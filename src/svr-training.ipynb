{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "\n",
    "Testing SVR Regression on Retrieval Task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3686731/1415649699.py:16: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n",
      "/home/staff_homes/kboenisc/miniconda3/envs/rapids-24.06/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cupy as cp\n",
    "import pandas as pd\n",
    "import torch, torch.nn.functional as F\n",
    "import os\n",
    "import gc \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import cudf\n",
    "import cuml\n",
    "import numba.cuda\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.spatial.distance import cdist\n",
    "from IPython.core.display import display, HTML\n",
    "from sklearn.manifold import TSNE\n",
    "from heapq import nlargest\n",
    "from io import StringIO\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoModel,AutoTokenizer\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from cuml.svm import SVR as cumlSVR\n",
    "from cuml.preprocessing import StandardScaler as cumlStandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "[<torch.cuda.device object at 0x7f17676de920>, <torch.cuda.device object at 0x7f17676dfc70>]\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "available_gpus = [torch.cuda.device(i) for i in range(torch.cuda.device_count())]\n",
    "print(device)\n",
    "print(available_gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    GERDALIR_PATH = './data/GerDaLIR/'\n",
    "    EMBEDDINGS_DIRECTORY = './data/embeddings/'\n",
    "    COMBINED_EMBEDDINGS = './data/embeddings/combined_{TYPE}_.npy'\n",
    "    REMOVE_STOPWORDS = False\n",
    "    EPXLODE_SENTENCES = True\n",
    "    USE_TFIDF = False\n",
    "    CHUNK_QUERIES = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tsv(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf8') as file:\n",
    "        for line in file:\n",
    "            columns = line.strip().split('\\t')\n",
    "            # Concatenate columns from index 1 onwards (since we have some broken rows)\n",
    "            concatenated_value = ''.join(columns[1:])\n",
    "            data.append((columns[0], concatenated_value))\n",
    "\n",
    "    df = pd.DataFrame(data, columns=['id', 'value'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = read_tsv(Config.GERDALIR_PATH + 'collection/collection.tsv').head(99900000)\n",
    "collection.rename(columns={'id': 'd_id', 'value':'passage'}, inplace=True)\n",
    "collection.reset_index(inplace=True)\n",
    "\n",
    "rels = read_tsv(Config.GERDALIR_PATH + 'qrels/qrels.train.tsv')\n",
    "rels.rename(columns={'id': 'q_id', 'value': 'd_id'}, inplace=True)\n",
    "rels = rels[rels['d_id'].isin(collection['d_id'].values)] # This can be deleted once we work with all\n",
    "rels.reset_index(inplace=True)\n",
    "\n",
    "queries = read_tsv(Config.GERDALIR_PATH + 'queries/queries.train.tsv')\n",
    "queries.rename(columns={'id': 'q_id', 'value': 'query'}, inplace=True)\n",
    "queries = queries[queries['q_id'].isin(rels['q_id'].values)] # This can be deleted once we work with all\n",
    "queries.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>q_id</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Nach [REF] ist eine Erlaubnis zu widerrufen, wenn nachträglich bekannt wird, dass die Voraussetzung nach § 0 Nummer 0 nicht erfüllt ist. Gemäß [REF] setzt die Erlaubnis zum Führen der Berufsbezeichnung voraus, dass die antragstellende Person sich nicht eines Verhaltens schuldig gemacht hat, aus dem sich die Unzuverlässigkeit zur Ausübung des Berufes ergibt. Der gerichtlich voll überprüfbare unbestimmte Rechtsbegriff der Zuverlässigkeit bezeichnet ein Instrument sicherheits und ordnungsrechtlicher Gefahrenabwehr. Der Ausschluss unzuverlässiger Erlaubnisbewerber bzw. inhaber hat demgemäß präventiven Charakter und dient der Abwehr von Gefahren für das Gemeinwohl. Unzuverlässigkeit i. S. d. der Bestimmungen ist dabei in Anlehnung an entsprechende Begrifflichkeiten in anderen, auch heilberufsrechtlichen Bestimmungen anzunehmen, wenn bei prognostischer Betrachtung auf Grund einer Würdigung der gesamten Persönlichkeit, des Gesamtverhaltens und der Lebensumstände des Betreffenden unter Berücksichtigung der Eigenart des Berufs nicht die Gewähr besteht, dass dieser in Zukunft seine beruflichen Pflichten zuverlässig erfüllen wird. Für die gebotene Prognose ist dabei abzustellen auf die jeweilige Situation des Betreffenden im maßgeblichen Zeitpunkt, der regelmäßig im Abschluss des behördlichen Verfahrens liegt, sowie auf vor allem durch die Art, Schwere und Zahl der Verstöße gegen die Berufspflichten manifest gewordenen Charakter des Betreffenden.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Erforderlich ist mithin eine Prognoseentscheidung unter Berücksichtigung aller Umstände des Einzelfalls dahingehend, ob der Betreffende willens und in der Lage sein wird, künftig seine beruflichen Pflichten zuverlässig zu erfüllen.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index q_id  \\\n",
       "0      0    2   \n",
       "1      1    3   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 query  \n",
       "0  Nach [REF] ist eine Erlaubnis zu widerrufen, wenn nachträglich bekannt wird, dass die Voraussetzung nach § 0 Nummer 0 nicht erfüllt ist. Gemäß [REF] setzt die Erlaubnis zum Führen der Berufsbezeichnung voraus, dass die antragstellende Person sich nicht eines Verhaltens schuldig gemacht hat, aus dem sich die Unzuverlässigkeit zur Ausübung des Berufes ergibt. Der gerichtlich voll überprüfbare unbestimmte Rechtsbegriff der Zuverlässigkeit bezeichnet ein Instrument sicherheits und ordnungsrechtlicher Gefahrenabwehr. Der Ausschluss unzuverlässiger Erlaubnisbewerber bzw. inhaber hat demgemäß präventiven Charakter und dient der Abwehr von Gefahren für das Gemeinwohl. Unzuverlässigkeit i. S. d. der Bestimmungen ist dabei in Anlehnung an entsprechende Begrifflichkeiten in anderen, auch heilberufsrechtlichen Bestimmungen anzunehmen, wenn bei prognostischer Betrachtung auf Grund einer Würdigung der gesamten Persönlichkeit, des Gesamtverhaltens und der Lebensumstände des Betreffenden unter Berücksichtigung der Eigenart des Berufs nicht die Gewähr besteht, dass dieser in Zukunft seine beruflichen Pflichten zuverlässig erfüllen wird. Für die gebotene Prognose ist dabei abzustellen auf die jeweilige Situation des Betreffenden im maßgeblichen Zeitpunkt, der regelmäßig im Abschluss des behördlichen Verfahrens liegt, sowie auf vor allem durch die Art, Schwere und Zahl der Verstöße gegen die Berufspflichten manifest gewordenen Charakter des Betreffenden.  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Erforderlich ist mithin eine Prognoseentscheidung unter Berücksichtigung aller Umstände des Einzelfalls dahingehend, ob der Betreffende willens und in der Lage sein wird, künftig seine beruflichen Pflichten zuverlässig zu erfüllen.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "98380"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>d_id</th>\n",
       "      <th>passage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Tenor Der Antrag der Klägerin auf Zulassung der Berufung gegen das Urteil des Verwaltungsgerichts Gelsenkirchen vom [DATE] wird abgelehnt. Die Klägerin trägt die Kosten des Zulassungsverfahrens. Der Streitwert wird auch für das Zulassungsverfahren auf 0 Euro festgesetzt. Gründe:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Das Zulassungsvorbringen der Klägerin begründet keine ernstlichen Zweifel an der Richtigkeit des angefochtenen Urteils . Zweifel in diesem Sinn sind anzunehmen, wenn ein einzelner tragender Rechtssatz oder eine einzelne erhebliche Tatsachenfeststellung des Verwaltungsgerichts mit schlüssigen Gegenargumenten in Frage gestellt werden.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index d_id  \\\n",
       "0      0    1   \n",
       "1      1    1   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                          passage  \n",
       "0                                                         Tenor Der Antrag der Klägerin auf Zulassung der Berufung gegen das Urteil des Verwaltungsgerichts Gelsenkirchen vom [DATE] wird abgelehnt. Die Klägerin trägt die Kosten des Zulassungsverfahrens. Der Streitwert wird auch für das Zulassungsverfahren auf 0 Euro festgesetzt. Gründe:  \n",
       "1  Das Zulassungsvorbringen der Klägerin begründet keine ernstlichen Zweifel an der Richtigkeit des angefochtenen Urteils . Zweifel in diesem Sinn sind anzunehmen, wenn ein einzelner tragender Rechtssatz oder eine einzelne erhebliche Tatsachenfeststellung des Verwaltungsgerichts mit schlüssigen Gegenargumenten in Frage gestellt werden.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3095383"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>q_id</th>\n",
       "      <th>d_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>118149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>72511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index q_id    d_id\n",
       "0      0    2  118149\n",
       "1      1    3   72511"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "115360"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(queries.head(2))\n",
    "display(len(queries))\n",
    "\n",
    "display(collection.head(2))\n",
    "display(len(collection))\n",
    "\n",
    "display(rels.head(2))\n",
    "display(len(rels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_string_with_overlap(s, k, overlap):\n",
    "    if k <= 0:\n",
    "        raise ValueError(\"Chunk length must be greater than 0\")\n",
    "    if overlap < 0:\n",
    "        raise ValueError(\"Overlap must be non-negative\")\n",
    "    if overlap >= k:\n",
    "        raise ValueError(\"Overlap must be less than chunk length\")\n",
    "    \n",
    "    chunks = []\n",
    "    for i in range(0, len(s), k - overlap):\n",
    "        chunk = s[i:i + k]\n",
    "        chunks.append(chunk)\n",
    "        if len(chunk) < k:\n",
    "            break\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average length of queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137.06715795893473\n",
      "1043.171305143322\n"
     ]
    }
   ],
   "source": [
    "print(queries['query'].apply(lambda x: len(x.split())).mean())\n",
    "print(queries['query'].apply(lambda x: len(x)).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(Config.CHUNK_QUERIES):\n",
    "    queries['query'] = queries['query'].apply(lambda q: chunk_string_with_overlap(q, 275, 15))\n",
    "    queries = queries.explode('query').reset_index(drop=True)\n",
    "    display(queries.head(10))\n",
    "    print(len(queries))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output.last_hidden_state.detach().cpu()\n",
    "    input_mask_expanded = (\n",
    "        attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    )\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(\n",
    "        input_mask_expanded.sum(1), min=1e-9\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length, text_col):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text_col = text_col\n",
    "        self.max = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df.loc[idx, self.text_col]\n",
    "        tokens = self.tokenizer(\n",
    "                text,\n",
    "                None,\n",
    "                add_special_tokens=True,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=self.max,\n",
    "                return_tensors=\"pt\")\n",
    "        tokens = {k:v.squeeze(0) for k,v in tokens.items()}\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbedderContainer:\n",
    "\n",
    "    def __init__(self, model_name, max_length, batch_size, device):\n",
    "        self.device = device\n",
    "        self.model_name = model_name\n",
    "        self.__model_path = model_name\n",
    "        self.__max_length = max_length\n",
    "        self.__batch_size = batch_size\n",
    "\n",
    "    def init_model(self):\n",
    "        self.__model = AutoModel.from_pretrained(self.__model_path, trust_remote_code=True).to(self.device)  #force_download=True\n",
    "        self.__model.eval()\n",
    "        self.__tokenizer = AutoTokenizer.from_pretrained(self.__model_path, trust_remote_code=True)  #force_download=True\n",
    "\n",
    "    def init_embed_dataset(self, df, text_col):\n",
    "        self.__dataset = EmbedDataset(df, self.__tokenizer, self.__max_length, text_col)\n",
    "        self.__embed_dataloader = torch.utils.data.DataLoader(self.__dataset,\n",
    "                                batch_size=self.__batch_size,\n",
    "                                shuffle=False)\n",
    "        return self.__embed_dataloader\n",
    "\n",
    "    def embed(self, input_ids, attention_mask):\n",
    "        return self.__model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    def dispose(self):\n",
    "        if hasattr(self, '__dataset'):\n",
    "            del self.__dataset\n",
    "        if hasattr(self, '__embed_dataloader'):\n",
    "            del self.__embed_dataloader\n",
    "        if hasattr(self, '__model'):\n",
    "            del self.__model\n",
    "        if hasattr(self, '__tokenizer'):\n",
    "            del self.__tokenizer\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(embedding_container, compute_collection=True, compute_queries=False):\n",
    "\n",
    "    global collection, queries\n",
    "    embedding_container.init_model()\n",
    "\n",
    "    # COMPUTE COLLECTION EMBEDDINGS\n",
    "    all_collection_embeddings = []\n",
    "    if compute_collection:\n",
    "        # Create dataset for collection\n",
    "        embed_dataloader_tr = embedding_container.init_embed_dataset(collection, 'passage')\n",
    "        for batch in tqdm(embed_dataloader_tr,total=len(embed_dataloader_tr)):\n",
    "            input_ids = batch[\"input_ids\"].to(embedding_container.device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(embedding_container.device)\n",
    "            with torch.no_grad():\n",
    "                with torch.cuda.amp.autocast(enabled=True):\n",
    "                    model_output = embedding_container.embed(input_ids, attention_mask)\n",
    "\n",
    "            sentence_embeddings = mean_pooling(model_output, attention_mask.detach().cpu())\n",
    "            # Normalize the embeddings\n",
    "            sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "            sentence_embeddings =  sentence_embeddings.squeeze(0).detach().cpu().numpy()\n",
    "            all_collection_embeddings.extend(sentence_embeddings)\n",
    "    all_collection_embeddings = np.array(all_collection_embeddings)\n",
    "\n",
    "    # COMPUTE QUERY EMBEDDINGS\n",
    "    all_query_embeddings = []\n",
    "    if compute_queries:\n",
    "        # Create dataset for query\n",
    "        embed_dataloader_te = embedding_container.init_embed_dataset(queries, 'query')\n",
    "        for batch in embed_dataloader_te:\n",
    "            input_ids = batch[\"input_ids\"].to(embedding_container.device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(embedding_container.device)\n",
    "            with torch.no_grad():\n",
    "                with torch.cuda.amp.autocast(enabled=True):\n",
    "                    model_output = embedding_container.embed(input_ids, attention_mask)\n",
    "            sentence_embeddings = mean_pooling(model_output, attention_mask.detach().cpu())\n",
    "            # Normalize the embeddings\n",
    "            sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "            sentence_embeddings =  sentence_embeddings.squeeze(0).detach().cpu().numpy()\n",
    "            all_query_embeddings.extend(sentence_embeddings)\n",
    "    all_query_embeddings = np.array(all_query_embeddings)\n",
    "\n",
    "    # DISPOSE\n",
    "    if 'model_output' in locals():\n",
    "        del model_output\n",
    "    if 'sentence_embeddings' in locals():\n",
    "        del sentence_embeddings\n",
    "    if 'input_ids' in locals():\n",
    "        del input_ids\n",
    "    if 'attention_mask' in locals():\n",
    "        del attention_mask\n",
    "    embedding_container.dispose()\n",
    "\n",
    "    # RETURN EMBEDDINGS\n",
    "    return all_collection_embeddings, all_query_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible Embedding Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda:1\" # We want to extract embeddings on the second GPU.\n",
    "\n",
    "container = [\n",
    "    #EmbedderContainer('microsoft/deberta-base', 1024, 32, DEVICE),\n",
    "    #EmbedderContainer('microsoft/deberta-large', 1024, 8, DEVICE),\n",
    "    #EmbedderContainer('microsoft/deberta-v3-large', 1024, 8, DEVICE),\n",
    "    EmbedderContainer('allenai/longformer-base-4096', 1024, 32, DEVICE),\n",
    "    #EmbedderContainer('LennartKeller/longformer-gottbert-base-8192-aw512', 1024, 32, DEVICE),\n",
    "    #EmbedderContainer('allenai/longformer-large-4096', 1024, 8, DEVICE),\n",
    "    #EmbedderContainer('google/bigbird-roberta-base', 1024, 32, DEVICE),\n",
    "    #EmbedderContainer('google/bigbird-roberta-large', 1024, 8, DEVICE),\n",
    "\n",
    "    # Sentence Transformers fine-tuned for sentence embeddings:\n",
    "    # EmbedderContainer('sentence-transformers/all-distilroberta-v1', 512, 32, DEVICE), # https://huggingface.co/sentence-transformers/all-distilroberta-v1\n",
    "    # EmbedderContainer('sentence-transformers/msmarco-distilbert-base-v4', 512, 32, DEVICE), # https://huggingface.co/sentence-transformers/msmarco-distilbert-base-v4\n",
    "    #EmbedderContainer('sentence-transformers/all-MiniLM-L6-v2', 384, 64, DEVICE), # https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_collection_embeds = []\n",
    "all_query_embeds = []\n",
    "\n",
    "calculate = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(os.path.exists(Config.COMBINED_EMBEDDINGS.replace('{TYPE}', 'collection_full'))):\n",
    "    all_collection_embeds = np.load(Config.COMBINED_EMBEDDINGS.replace('{TYPE}', 'collection_full'))\n",
    "    all_query_embeds = np.load(Config.COMBINED_EMBEDDINGS.replace('{TYPE}', 'query_train'))\n",
    "    calculate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings for ./data/embeddings/collection_allenai_longformer-base-4096.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/staff_homes/kboenisc/miniconda3/envs/rapids-24.06/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "if(calculate):\n",
    "    for embedding_container in container:\n",
    "        name = (Config.EMBEDDINGS_DIRECTORY + \n",
    "                'collection_' + \n",
    "                embedding_container.model_name.replace(\"/\", \"_\") + \n",
    "                \".npy\")\n",
    "        \n",
    "        if os.path.exists(name):\n",
    "            print(f\"Loading embeddings for {name}\")\n",
    "            _, query_embed = get_embeddings(embedding_container, compute_collection=False, compute_queries=True)\n",
    "            collection_embed = np.load(name)\n",
    "        else:\n",
    "            print(f\"Computing embeddings for {name}\") \n",
    "            collection_embed, query_embed = get_embeddings(embedding_container, compute_collection=True, compute_queries=True)\n",
    "            np.save(name, collection_embed)\n",
    "        all_collection_embeds.append(collection_embed)\n",
    "        all_query_embeds.append(query_embed)\n",
    "\n",
    "    del collection_embed, query_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(calculate):\n",
    "    all_collection_embeds = np.concatenate(all_collection_embeds,axis=1)\n",
    "    all_query_embeds = np.concatenate(all_query_embeds,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3095383, 768)\n",
      "(98380, 768)\n"
     ]
    }
   ],
   "source": [
    "print(all_collection_embeds.shape)\n",
    "print(all_query_embeds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(calculate):\n",
    "    np.save(Config.COMBINED_EMBEDDINGS.replace('{TYPE}', 'collection_full'), all_collection_embeds)\n",
    "    np.save(Config.COMBINED_EMBEDDINGS.replace('{TYPE}', 'query_train'), all_query_embeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "labels = []\n",
    "calculate_feat = True\n",
    "\n",
    "if(os.path.exists(Config.COMBINED_EMBEDDINGS.replace('{TYPE}', 'features'))):\n",
    "    features = np.load(Config.COMBINED_EMBEDDINGS.replace('{TYPE}', 'features'))\n",
    "    labels = np.load(Config.COMBINED_EMBEDDINGS.replace('{TYPE}', 'labels'))\n",
    "    calculate_feat = False\n",
    "\n",
    "subsampling_top_k = 100\n",
    "\n",
    "# query_idx_lookup = {qid: idx for idx, qid in enumerate(queries['q_id'])}\n",
    "passage_idx_lookup = {pid: idx for idx, pid in enumerate(collection['d_id'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_fits_passage(q_idx, c_idx):\n",
    "    '''Does the given collection index fit to a given query index?'''\n",
    "    q_id = queries.iloc[q_idx]['q_id']\n",
    "    d_id = collection.iloc[c_idx]['d_id']\n",
    "    return len(rels[(rels['q_id'] == q_id) & (rels['d_id'] == d_id)].values) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert to cupy arrays for faster GPU cosine sims**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(calculate_feat):\n",
    "    all_query_embeds_cp = cp.asarray(all_query_embeds, dtype=cp.float16) \n",
    "    all_collection_embeds_cp = cp.asarray(all_collection_embeds, dtype=cp.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Queries: 100%|█████████████████████████████████████████████████████████| 98380/98380 [45:52<00:00, 35.75it/s]\n"
     ]
    }
   ],
   "source": [
    "# Loop through queries\n",
    "if(calculate_feat):\n",
    "    for q_idx, row in tqdm(queries.iterrows(), total=len(queries), desc='Processing Queries'):\n",
    "        query_id = row['q_id']\n",
    "        query_embed_cp = all_query_embeds_cp[q_idx]\n",
    "\n",
    "        # Positive samples\n",
    "        positive_indices = set()\n",
    "        rels_for_query = rels[rels['q_id'] == query_id]\n",
    "        for passage_id in rels_for_query['d_id']:\n",
    "            if passage_id in passage_idx_lookup:\n",
    "                passage_idx = passage_idx_lookup[passage_id]\n",
    "                #print('Correct answer:')\n",
    "                #print(collection.iloc[passage_idx]['passage'])\n",
    "                passage_embed_cp = all_collection_embeds_cp[passage_idx]\n",
    "\n",
    "                # Convert CuPy arrays back to NumPy\n",
    "                query_embed_np = cp.asnumpy(query_embed_cp)\n",
    "                passage_embed_np = cp.asnumpy(passage_embed_cp)\n",
    "\n",
    "                # Concatenate using NumPy\n",
    "                feature_np = np.concatenate((query_embed_np, passage_embed_np))\n",
    "                features.append(feature_np)  # Append NumPy array\n",
    "                labels.append(1)\n",
    "                positive_indices.add(passage_idx)\n",
    "        \n",
    "        # Compute cosine similarity on GPU\n",
    "        cos_sim_cp = 1 - cp.matmul(query_embed_cp, all_collection_embeds_cp.T)\n",
    "        top_k_indices = cp.asnumpy(cp.argpartition(cos_sim_cp, subsampling_top_k + len(positive_indices))[:subsampling_top_k + len(positive_indices)])\n",
    "        negative_indices = top_k_indices[:subsampling_top_k]\n",
    "        #print('Query')\n",
    "        #print(queries.iloc[q_idx]['query'])\n",
    "        #print('Passage')\n",
    "        #print(collection.iloc[negative_indices[0]]['passage'])\n",
    "        #print('\\n\\n')\n",
    "\n",
    "        for passage_idx in negative_indices:\n",
    "            passage_embed_cp = all_collection_embeds_cp[passage_idx]\n",
    "\n",
    "            # Convert CuPy arrays back to NumPy\n",
    "            query_embed_np = cp.asnumpy(query_embed_cp)\n",
    "            passage_embed_np = cp.asnumpy(passage_embed_cp)\n",
    "\n",
    "            # Concatenate using NumPy\n",
    "            feature_np = np.concatenate((query_embed_np, passage_embed_np)).astype(np.float16)\n",
    "            features.append(feature_np)  # Append NumPy array\n",
    "            labels.append(0)\n",
    "\n",
    "        # Explicitly free memory if needed\n",
    "        # cp.get_default_memory_pool().free_all_blocks()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(calculate_feat):\n",
    "    features = np.array(features)\n",
    "    labels = np.array(labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float16\n"
     ]
    }
   ],
   "source": [
    "print(features.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 5] Input/output error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(calculate_feat):\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOMBINED_EMBEDDINGS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{TYPE}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfeatures\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     np\u001b[38;5;241m.\u001b[39msave(Config\u001b[38;5;241m.\u001b[39mCOMBINED_EMBEDDINGS\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{TYPE}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m), labels)\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids-24.06/lib/python3.10/site-packages/numpy/lib/npyio.py:544\u001b[0m, in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    541\u001b[0m         file \u001b[38;5;241m=\u001b[39m file \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    542\u001b[0m     file_ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 544\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m file_ctx \u001b[38;5;28;01mas\u001b[39;00m fid:\n\u001b[1;32m    545\u001b[0m     arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(arr)\n\u001b[1;32m    546\u001b[0m     \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_array(fid, arr, allow_pickle\u001b[38;5;241m=\u001b[39mallow_pickle,\n\u001b[1;32m    547\u001b[0m                        pickle_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(fix_imports\u001b[38;5;241m=\u001b[39mfix_imports))\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 5] Input/output error"
     ]
    }
   ],
   "source": [
    "if(calculate_feat):\n",
    "    np.save(Config.COMBINED_EMBEDDINGS.replace('{TYPE}', 'features'), features)\n",
    "    np.save(Config.COMBINED_EMBEDDINGS.replace('{TYPE}', 'labels'), labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure there are no NaNs or infinities in the data\n",
    "#if np.any(np.isnan(features)) or np.any(np.isinf(features)):\n",
    "#    raise ValueError(\"Input data contains NaNs or infinite values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9953360, 1536)\n",
      "(9953360,)\n",
      "[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Negative labels: 9838000\n",
      "Positive labels: 115360\n"
     ]
    }
   ],
   "source": [
    "print(features.shape)\n",
    "print(labels.shape)\n",
    "print(labels[:200])\n",
    "print('Negative labels: ' + str(len(list(filter(lambda x: x == 0, labels)))))\n",
    "print('Positive labels: ' + str(len(list(filter(lambda x: x > 0, labels)))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'all_collection_embeds' in locals() and 'all_collection_embeds_cp' in locals():\n",
    "    del all_collection_embeds, all_collection_embeds_cp, all_query_embeds, all_query_embeds_cp\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8958024\n",
      "8958024\n",
      "8958024\n",
      "995336\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "print(len(X_train))\n",
    "print(len(y_train))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del features, labels\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Even with subsampling, we still have a class imbalance. Counter that:**\n",
    "\n",
    "*Update*: Yikes, RAPIDS doesn't support passing the class_weights into the SVR yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute class weights to handle imbalance\n",
    "# class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "# class_weights_dict = {i: class_weights[i] for i in range(len(class_weights))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging\n",
    "\n",
    "We have too many features and datapoints, which a single SVR can't handle on this machine. Instead, I'm going to:\n",
    "\n",
    "- Train X different SVR using a portion of the rows\n",
    "- Ensemble the SVRs\n",
    "- In that way duplicate the bagging of Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to create random subsets with overlap\n",
    "def create_subsets(X, y, num_subsets, overlap_percentage):\n",
    "    subsets = []\n",
    "    subset_size = int((len(X) // num_subsets) * overlap_percentage)\n",
    "    num_samples = len(X)\n",
    "    \n",
    "    for _ in range(num_subsets):\n",
    "        subset_indices = np.random.choice(num_samples, size=subset_size, replace=True)\n",
    "        X_subset = X[subset_indices]\n",
    "        y_subset = y[subset_indices]\n",
    "        subsets.append((X_subset, y_subset))\n",
    "        print(f'Done with {_}/{num_subsets}')\n",
    "    \n",
    "    return subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging function to track progress\n",
    "def log_progress(model, X_val, y_val):\n",
    "    y_pred = model.predict(X_val)\n",
    "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "    \n",
    "    precision = precision_score(y_val, y_pred_binary)\n",
    "    recall = recall_score(y_val, y_pred_binary)\n",
    "    f1 = f1_score(y_val, y_pred_binary)\n",
    "    accuracy = accuracy_score(y_val, y_pred_binary)\n",
    "    \n",
    "    print(f'  Accuracy: {accuracy:.4f}')\n",
    "    print(f'  Precision: {precision:.4f}')\n",
    "    print(f'  Recall: {recall:.4f}')\n",
    "    print(f'  F1 Score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize\n",
    "\n",
    "There is no way we fit 200GB of features into the GPU at once to transform the data. Hence, we calculate the fitting on the CPU with the standard scaler, chunk it and then transform in chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing Chunk: 0/358\n",
      "Doing Chunk: 1/358\n",
      "Doing Chunk: 2/358\n",
      "Doing Chunk: 3/358\n",
      "Doing Chunk: 4/358\n",
      "Doing Chunk: 5/358\n",
      "Doing Chunk: 6/358\n",
      "Doing Chunk: 7/358\n",
      "Doing Chunk: 8/358\n",
      "Doing Chunk: 9/358\n",
      "Doing Chunk: 10/358\n",
      "Doing Chunk: 11/358\n",
      "Doing Chunk: 12/358\n",
      "Doing Chunk: 13/358\n",
      "Doing Chunk: 14/358\n",
      "Doing Chunk: 15/358\n",
      "Doing Chunk: 16/358\n",
      "Doing Chunk: 17/358\n",
      "Doing Chunk: 18/358\n",
      "Doing Chunk: 19/358\n",
      "Doing Chunk: 20/358\n",
      "Doing Chunk: 21/358\n",
      "Doing Chunk: 22/358\n",
      "Doing Chunk: 23/358\n",
      "Doing Chunk: 24/358\n",
      "Doing Chunk: 25/358\n",
      "Doing Chunk: 26/358\n",
      "Doing Chunk: 27/358\n",
      "Doing Chunk: 28/358\n",
      "Doing Chunk: 29/358\n",
      "Doing Chunk: 30/358\n",
      "Doing Chunk: 31/358\n",
      "Doing Chunk: 32/358\n",
      "Doing Chunk: 33/358\n",
      "Doing Chunk: 34/358\n",
      "Doing Chunk: 35/358\n",
      "Doing Chunk: 36/358\n",
      "Doing Chunk: 37/358\n",
      "Doing Chunk: 38/358\n",
      "Doing Chunk: 39/358\n",
      "Doing Chunk: 40/358\n",
      "Doing Chunk: 41/358\n",
      "Doing Chunk: 42/358\n",
      "Doing Chunk: 43/358\n",
      "Doing Chunk: 44/358\n",
      "Doing Chunk: 45/358\n",
      "Doing Chunk: 46/358\n",
      "Doing Chunk: 47/358\n",
      "Doing Chunk: 48/358\n",
      "Doing Chunk: 49/358\n",
      "Doing Chunk: 50/358\n",
      "Doing Chunk: 51/358\n",
      "Doing Chunk: 52/358\n",
      "Doing Chunk: 53/358\n",
      "Doing Chunk: 54/358\n",
      "Doing Chunk: 55/358\n",
      "Doing Chunk: 56/358\n",
      "Doing Chunk: 57/358\n",
      "Doing Chunk: 58/358\n",
      "Doing Chunk: 59/358\n",
      "Doing Chunk: 60/358\n",
      "Doing Chunk: 61/358\n",
      "Doing Chunk: 62/358\n",
      "Doing Chunk: 63/358\n",
      "Doing Chunk: 64/358\n",
      "Doing Chunk: 65/358\n",
      "Doing Chunk: 66/358\n",
      "Doing Chunk: 67/358\n",
      "Doing Chunk: 68/358\n",
      "Doing Chunk: 69/358\n",
      "Doing Chunk: 70/358\n",
      "Doing Chunk: 71/358\n",
      "Doing Chunk: 72/358\n",
      "Doing Chunk: 73/358\n",
      "Doing Chunk: 74/358\n",
      "Doing Chunk: 75/358\n",
      "Doing Chunk: 76/358\n",
      "Doing Chunk: 77/358\n",
      "Doing Chunk: 78/358\n",
      "Doing Chunk: 79/358\n",
      "Doing Chunk: 80/358\n",
      "Doing Chunk: 81/358\n",
      "Doing Chunk: 82/358\n",
      "Doing Chunk: 83/358\n",
      "Doing Chunk: 84/358\n",
      "Doing Chunk: 85/358\n",
      "Doing Chunk: 86/358\n",
      "Doing Chunk: 87/358\n",
      "Doing Chunk: 88/358\n",
      "Doing Chunk: 89/358\n",
      "Doing Chunk: 90/358\n",
      "Doing Chunk: 91/358\n",
      "Doing Chunk: 92/358\n",
      "Doing Chunk: 93/358\n",
      "Doing Chunk: 94/358\n",
      "Doing Chunk: 95/358\n",
      "Doing Chunk: 96/358\n",
      "Doing Chunk: 97/358\n",
      "Doing Chunk: 98/358\n",
      "Doing Chunk: 99/358\n",
      "Doing Chunk: 100/358\n",
      "Doing Chunk: 101/358\n",
      "Doing Chunk: 102/358\n",
      "Doing Chunk: 103/358\n",
      "Doing Chunk: 104/358\n",
      "Doing Chunk: 105/358\n",
      "Doing Chunk: 106/358\n",
      "Doing Chunk: 107/358\n",
      "Doing Chunk: 108/358\n",
      "Doing Chunk: 109/358\n",
      "Doing Chunk: 110/358\n",
      "Doing Chunk: 111/358\n",
      "Doing Chunk: 112/358\n",
      "Doing Chunk: 113/358\n",
      "Doing Chunk: 114/358\n",
      "Doing Chunk: 115/358\n",
      "Doing Chunk: 116/358\n",
      "Doing Chunk: 117/358\n",
      "Doing Chunk: 118/358\n",
      "Doing Chunk: 119/358\n",
      "Doing Chunk: 120/358\n",
      "Doing Chunk: 121/358\n",
      "Doing Chunk: 122/358\n",
      "Doing Chunk: 123/358\n",
      "Doing Chunk: 124/358\n",
      "Doing Chunk: 125/358\n",
      "Doing Chunk: 126/358\n",
      "Doing Chunk: 127/358\n",
      "Doing Chunk: 128/358\n",
      "Doing Chunk: 129/358\n",
      "Doing Chunk: 130/358\n",
      "Doing Chunk: 131/358\n",
      "Doing Chunk: 132/358\n",
      "Doing Chunk: 133/358\n",
      "Doing Chunk: 134/358\n",
      "Doing Chunk: 135/358\n",
      "Doing Chunk: 136/358\n",
      "Doing Chunk: 137/358\n",
      "Doing Chunk: 138/358\n",
      "Doing Chunk: 139/358\n",
      "Doing Chunk: 140/358\n",
      "Doing Chunk: 141/358\n",
      "Doing Chunk: 142/358\n",
      "Doing Chunk: 143/358\n",
      "Doing Chunk: 144/358\n",
      "Doing Chunk: 145/358\n",
      "Doing Chunk: 146/358\n",
      "Doing Chunk: 147/358\n",
      "Doing Chunk: 148/358\n",
      "Doing Chunk: 149/358\n",
      "Doing Chunk: 150/358\n",
      "Doing Chunk: 151/358\n",
      "Doing Chunk: 152/358\n",
      "Doing Chunk: 153/358\n",
      "Doing Chunk: 154/358\n",
      "Doing Chunk: 155/358\n",
      "Doing Chunk: 156/358\n",
      "Doing Chunk: 157/358\n",
      "Doing Chunk: 158/358\n",
      "Doing Chunk: 159/358\n",
      "Doing Chunk: 160/358\n",
      "Doing Chunk: 161/358\n",
      "Doing Chunk: 162/358\n",
      "Doing Chunk: 163/358\n",
      "Doing Chunk: 164/358\n",
      "Doing Chunk: 165/358\n",
      "Doing Chunk: 166/358\n",
      "Doing Chunk: 167/358\n",
      "Doing Chunk: 168/358\n",
      "Doing Chunk: 169/358\n",
      "Doing Chunk: 170/358\n",
      "Doing Chunk: 171/358\n",
      "Doing Chunk: 172/358\n",
      "Doing Chunk: 173/358\n",
      "Doing Chunk: 174/358\n",
      "Doing Chunk: 175/358\n",
      "Doing Chunk: 176/358\n",
      "Doing Chunk: 177/358\n",
      "Doing Chunk: 178/358\n",
      "Doing Chunk: 179/358\n",
      "Doing Chunk: 180/358\n",
      "Doing Chunk: 181/358\n",
      "Doing Chunk: 182/358\n",
      "Doing Chunk: 183/358\n",
      "Doing Chunk: 184/358\n",
      "Doing Chunk: 185/358\n",
      "Doing Chunk: 186/358\n",
      "Doing Chunk: 187/358\n",
      "Doing Chunk: 188/358\n",
      "Doing Chunk: 189/358\n",
      "Doing Chunk: 190/358\n",
      "Doing Chunk: 191/358\n",
      "Doing Chunk: 192/358\n",
      "Doing Chunk: 193/358\n",
      "Doing Chunk: 194/358\n",
      "Doing Chunk: 195/358\n",
      "Doing Chunk: 196/358\n",
      "Doing Chunk: 197/358\n",
      "Doing Chunk: 198/358\n",
      "Doing Chunk: 199/358\n",
      "Doing Chunk: 200/358\n",
      "Doing Chunk: 201/358\n",
      "Doing Chunk: 202/358\n",
      "Doing Chunk: 203/358\n",
      "Doing Chunk: 204/358\n",
      "Doing Chunk: 205/358\n",
      "Doing Chunk: 206/358\n",
      "Doing Chunk: 207/358\n",
      "Doing Chunk: 208/358\n",
      "Doing Chunk: 209/358\n",
      "Doing Chunk: 210/358\n",
      "Doing Chunk: 211/358\n",
      "Doing Chunk: 212/358\n",
      "Doing Chunk: 213/358\n",
      "Doing Chunk: 214/358\n",
      "Doing Chunk: 215/358\n",
      "Doing Chunk: 216/358\n",
      "Doing Chunk: 217/358\n",
      "Doing Chunk: 218/358\n",
      "Doing Chunk: 219/358\n",
      "Doing Chunk: 220/358\n",
      "Doing Chunk: 221/358\n",
      "Doing Chunk: 222/358\n",
      "Doing Chunk: 223/358\n",
      "Doing Chunk: 224/358\n",
      "Doing Chunk: 225/358\n",
      "Doing Chunk: 226/358\n",
      "Doing Chunk: 227/358\n",
      "Doing Chunk: 228/358\n",
      "Doing Chunk: 229/358\n",
      "Doing Chunk: 230/358\n",
      "Doing Chunk: 231/358\n",
      "Doing Chunk: 232/358\n",
      "Doing Chunk: 233/358\n",
      "Doing Chunk: 234/358\n",
      "Doing Chunk: 235/358\n",
      "Doing Chunk: 236/358\n",
      "Doing Chunk: 237/358\n",
      "Doing Chunk: 238/358\n",
      "Doing Chunk: 239/358\n",
      "Doing Chunk: 240/358\n",
      "Doing Chunk: 241/358\n",
      "Doing Chunk: 242/358\n",
      "Doing Chunk: 243/358\n",
      "Doing Chunk: 244/358\n",
      "Doing Chunk: 245/358\n",
      "Doing Chunk: 246/358\n",
      "Doing Chunk: 247/358\n",
      "Doing Chunk: 248/358\n",
      "Doing Chunk: 249/358\n",
      "Doing Chunk: 250/358\n",
      "Doing Chunk: 251/358\n",
      "Doing Chunk: 252/358\n",
      "Doing Chunk: 253/358\n",
      "Doing Chunk: 254/358\n",
      "Doing Chunk: 255/358\n",
      "Doing Chunk: 256/358\n",
      "Doing Chunk: 257/358\n",
      "Doing Chunk: 258/358\n",
      "Doing Chunk: 259/358\n",
      "Doing Chunk: 260/358\n",
      "Doing Chunk: 261/358\n",
      "Doing Chunk: 262/358\n",
      "Doing Chunk: 263/358\n",
      "Doing Chunk: 264/358\n",
      "Doing Chunk: 265/358\n",
      "Doing Chunk: 266/358\n",
      "Doing Chunk: 267/358\n",
      "Doing Chunk: 268/358\n",
      "Doing Chunk: 269/358\n",
      "Doing Chunk: 270/358\n",
      "Doing Chunk: 271/358\n",
      "Doing Chunk: 272/358\n",
      "Doing Chunk: 273/358\n",
      "Doing Chunk: 274/358\n",
      "Doing Chunk: 275/358\n",
      "Doing Chunk: 276/358\n",
      "Doing Chunk: 277/358\n",
      "Doing Chunk: 278/358\n",
      "Doing Chunk: 279/358\n",
      "Doing Chunk: 280/358\n",
      "Doing Chunk: 281/358\n",
      "Doing Chunk: 282/358\n",
      "Doing Chunk: 283/358\n",
      "Doing Chunk: 284/358\n",
      "Doing Chunk: 285/358\n",
      "Doing Chunk: 286/358\n",
      "Doing Chunk: 287/358\n",
      "Doing Chunk: 288/358\n",
      "Doing Chunk: 289/358\n",
      "Doing Chunk: 290/358\n",
      "Doing Chunk: 291/358\n",
      "Doing Chunk: 292/358\n",
      "Doing Chunk: 293/358\n",
      "Doing Chunk: 294/358\n",
      "Doing Chunk: 295/358\n",
      "Doing Chunk: 296/358\n",
      "Doing Chunk: 297/358\n",
      "Doing Chunk: 298/358\n",
      "Doing Chunk: 299/358\n",
      "Doing Chunk: 300/358\n",
      "Doing Chunk: 301/358\n",
      "Doing Chunk: 302/358\n",
      "Doing Chunk: 303/358\n",
      "Doing Chunk: 304/358\n",
      "Doing Chunk: 305/358\n",
      "Doing Chunk: 306/358\n",
      "Doing Chunk: 307/358\n",
      "Doing Chunk: 308/358\n",
      "Doing Chunk: 309/358\n",
      "Doing Chunk: 310/358\n",
      "Doing Chunk: 311/358\n",
      "Doing Chunk: 312/358\n",
      "Doing Chunk: 313/358\n",
      "Doing Chunk: 314/358\n",
      "Doing Chunk: 315/358\n",
      "Doing Chunk: 316/358\n",
      "Doing Chunk: 317/358\n",
      "Doing Chunk: 318/358\n",
      "Doing Chunk: 319/358\n",
      "Doing Chunk: 320/358\n",
      "Doing Chunk: 321/358\n",
      "Doing Chunk: 322/358\n",
      "Doing Chunk: 323/358\n",
      "Doing Chunk: 324/358\n",
      "Doing Chunk: 325/358\n",
      "Doing Chunk: 326/358\n",
      "Doing Chunk: 327/358\n",
      "Doing Chunk: 328/358\n",
      "Doing Chunk: 329/358\n",
      "Doing Chunk: 330/358\n",
      "Doing Chunk: 331/358\n",
      "Doing Chunk: 332/358\n",
      "Doing Chunk: 333/358\n",
      "Doing Chunk: 334/358\n",
      "Doing Chunk: 335/358\n",
      "Doing Chunk: 336/358\n",
      "Doing Chunk: 337/358\n",
      "Doing Chunk: 338/358\n",
      "Doing Chunk: 339/358\n",
      "Doing Chunk: 340/358\n",
      "Doing Chunk: 341/358\n",
      "Doing Chunk: 342/358\n",
      "Doing Chunk: 343/358\n",
      "Doing Chunk: 344/358\n",
      "Doing Chunk: 345/358\n",
      "Doing Chunk: 346/358\n",
      "Doing Chunk: 347/358\n",
      "Doing Chunk: 348/358\n",
      "Doing Chunk: 349/358\n",
      "Doing Chunk: 350/358\n",
      "Doing Chunk: 351/358\n",
      "Doing Chunk: 352/358\n",
      "Doing Chunk: 353/358\n",
      "Doing Chunk: 354/358\n",
      "Doing Chunk: 355/358\n",
      "Doing Chunk: 356/358\n",
      "Doing Chunk: 357/358\n",
      "Doing Chunk: 358/358\n"
     ]
    }
   ],
   "source": [
    "def chunkify(data, chunk_size):\n",
    "    for i in range(0, len(data), chunk_size):\n",
    "        yield data[i:i + chunk_size]\n",
    "\n",
    "# Initialize a CPU-based StandardScaler\n",
    "cpu_scaler = StandardScaler()\n",
    "\n",
    "# Fit the CPU scaler incrementally on each chunk of X_train\n",
    "chunk_size = 25000\n",
    "count = 0\n",
    "total = len(X_train) // chunk_size\n",
    "for chunk in chunkify(X_train, chunk_size):\n",
    "    print(f'Doing Chunk: {count}/{total}')\n",
    "    cpu_scaler.partial_fit(chunk)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save the scaler for the test inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cpu_scaler.pkl', 'wb') as file:\n",
    "    pickle.dump(cpu_scaler, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 0/8958024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 25000/8958024\n",
      "Done with 50000/8958024\n",
      "Done with 75000/8958024\n",
      "Done with 100000/8958024\n",
      "Done with 125000/8958024\n",
      "Done with 150000/8958024\n",
      "Done with 175000/8958024\n",
      "Done with 200000/8958024\n",
      "Done with 225000/8958024\n",
      "Done with 250000/8958024\n",
      "Done with 275000/8958024\n",
      "Done with 300000/8958024\n",
      "Done with 325000/8958024\n",
      "Done with 350000/8958024\n",
      "Done with 375000/8958024\n",
      "Done with 400000/8958024\n",
      "Done with 425000/8958024\n",
      "Done with 450000/8958024\n",
      "Done with 475000/8958024\n",
      "Done with 500000/8958024\n",
      "Done with 525000/8958024\n",
      "Done with 550000/8958024\n",
      "Done with 575000/8958024\n",
      "Done with 600000/8958024\n",
      "Done with 625000/8958024\n",
      "Done with 650000/8958024\n",
      "Done with 675000/8958024\n",
      "Done with 700000/8958024\n",
      "Done with 725000/8958024\n",
      "Done with 750000/8958024\n",
      "Done with 775000/8958024\n",
      "Done with 800000/8958024\n",
      "Done with 825000/8958024\n",
      "Done with 850000/8958024\n",
      "Done with 875000/8958024\n",
      "Done with 900000/8958024\n",
      "Done with 925000/8958024\n",
      "Done with 950000/8958024\n",
      "Done with 975000/8958024\n",
      "Done with 1000000/8958024\n",
      "Done with 1025000/8958024\n",
      "Done with 1050000/8958024\n",
      "Done with 1075000/8958024\n",
      "Done with 1100000/8958024\n",
      "Done with 1125000/8958024\n",
      "Done with 1150000/8958024\n",
      "Done with 1175000/8958024\n",
      "Done with 1200000/8958024\n",
      "Done with 1225000/8958024\n",
      "Done with 1250000/8958024\n",
      "Done with 1275000/8958024\n",
      "Done with 1300000/8958024\n",
      "Done with 1325000/8958024\n",
      "Done with 1350000/8958024\n",
      "Done with 1375000/8958024\n",
      "Done with 1400000/8958024\n",
      "Done with 1425000/8958024\n",
      "Done with 1450000/8958024\n",
      "Done with 1475000/8958024\n",
      "Done with 1500000/8958024\n",
      "Done with 1525000/8958024\n",
      "Done with 1550000/8958024\n",
      "Done with 1575000/8958024\n",
      "Done with 1600000/8958024\n",
      "Done with 1625000/8958024\n",
      "Done with 1650000/8958024\n",
      "Done with 1675000/8958024\n",
      "Done with 1700000/8958024\n",
      "Done with 1725000/8958024\n",
      "Done with 1750000/8958024\n",
      "Done with 1775000/8958024\n",
      "Done with 1800000/8958024\n",
      "Done with 1825000/8958024\n",
      "Done with 1850000/8958024\n",
      "Done with 1875000/8958024\n",
      "Done with 1900000/8958024\n",
      "Done with 1925000/8958024\n",
      "Done with 1950000/8958024\n",
      "Done with 1975000/8958024\n",
      "Done with 2000000/8958024\n",
      "Done with 2025000/8958024\n",
      "Done with 2050000/8958024\n",
      "Done with 2075000/8958024\n",
      "Done with 2100000/8958024\n",
      "Done with 2125000/8958024\n",
      "Done with 2150000/8958024\n",
      "Done with 2175000/8958024\n",
      "Done with 2200000/8958024\n",
      "Done with 2225000/8958024\n",
      "Done with 2250000/8958024\n",
      "Done with 2275000/8958024\n",
      "Done with 2300000/8958024\n",
      "Done with 2325000/8958024\n",
      "Done with 2350000/8958024\n",
      "Done with 2375000/8958024\n",
      "Done with 2400000/8958024\n",
      "Done with 2425000/8958024\n",
      "Done with 2450000/8958024\n",
      "Done with 2475000/8958024\n",
      "Done with 2500000/8958024\n",
      "Done with 2525000/8958024\n",
      "Done with 2550000/8958024\n",
      "Done with 2575000/8958024\n",
      "Done with 2600000/8958024\n",
      "Done with 2625000/8958024\n",
      "Done with 2650000/8958024\n",
      "Done with 2675000/8958024\n",
      "Done with 2700000/8958024\n",
      "Done with 2725000/8958024\n",
      "Done with 2750000/8958024\n",
      "Done with 2775000/8958024\n",
      "Done with 2800000/8958024\n",
      "Done with 2825000/8958024\n",
      "Done with 2850000/8958024\n",
      "Done with 2875000/8958024\n",
      "Done with 2900000/8958024\n",
      "Done with 2925000/8958024\n",
      "Done with 2950000/8958024\n",
      "Done with 2975000/8958024\n",
      "Done with 3000000/8958024\n",
      "Done with 3025000/8958024\n",
      "Done with 3050000/8958024\n",
      "Done with 3075000/8958024\n",
      "Done with 3100000/8958024\n",
      "Done with 3125000/8958024\n",
      "Done with 3150000/8958024\n",
      "Done with 3175000/8958024\n",
      "Done with 3200000/8958024\n",
      "Done with 3225000/8958024\n",
      "Done with 3250000/8958024\n",
      "Done with 3275000/8958024\n",
      "Done with 3300000/8958024\n",
      "Done with 3325000/8958024\n",
      "Done with 3350000/8958024\n",
      "Done with 3375000/8958024\n",
      "Done with 3400000/8958024\n",
      "Done with 3425000/8958024\n",
      "Done with 3450000/8958024\n",
      "Done with 3475000/8958024\n",
      "Done with 3500000/8958024\n",
      "Done with 3525000/8958024\n",
      "Done with 3550000/8958024\n",
      "Done with 3575000/8958024\n",
      "Done with 3600000/8958024\n",
      "Done with 3625000/8958024\n",
      "Done with 3650000/8958024\n",
      "Done with 3675000/8958024\n",
      "Done with 3700000/8958024\n",
      "Done with 3725000/8958024\n",
      "Done with 3750000/8958024\n",
      "Done with 3775000/8958024\n",
      "Done with 3800000/8958024\n",
      "Done with 3825000/8958024\n",
      "Done with 3850000/8958024\n",
      "Done with 3875000/8958024\n",
      "Done with 3900000/8958024\n",
      "Done with 3925000/8958024\n",
      "Done with 3950000/8958024\n",
      "Done with 3975000/8958024\n",
      "Done with 4000000/8958024\n",
      "Done with 4025000/8958024\n",
      "Done with 4050000/8958024\n",
      "Done with 4075000/8958024\n",
      "Done with 4100000/8958024\n",
      "Done with 4125000/8958024\n",
      "Done with 4150000/8958024\n",
      "Done with 4175000/8958024\n",
      "Done with 4200000/8958024\n",
      "Done with 4225000/8958024\n",
      "Done with 4250000/8958024\n",
      "Done with 4275000/8958024\n",
      "Done with 4300000/8958024\n",
      "Done with 4325000/8958024\n",
      "Done with 4350000/8958024\n",
      "Done with 4375000/8958024\n",
      "Done with 4400000/8958024\n",
      "Done with 4425000/8958024\n",
      "Done with 4450000/8958024\n",
      "Done with 4475000/8958024\n",
      "Done with 4500000/8958024\n",
      "Done with 4525000/8958024\n",
      "Done with 4550000/8958024\n",
      "Done with 4575000/8958024\n",
      "Done with 4600000/8958024\n",
      "Done with 4625000/8958024\n",
      "Done with 4650000/8958024\n",
      "Done with 4675000/8958024\n",
      "Done with 4700000/8958024\n",
      "Done with 4725000/8958024\n",
      "Done with 4750000/8958024\n",
      "Done with 4775000/8958024\n",
      "Done with 4800000/8958024\n",
      "Done with 4825000/8958024\n",
      "Done with 4850000/8958024\n",
      "Done with 4875000/8958024\n",
      "Done with 4900000/8958024\n",
      "Done with 4925000/8958024\n",
      "Done with 4950000/8958024\n",
      "Done with 4975000/8958024\n",
      "Done with 5000000/8958024\n",
      "Done with 5025000/8958024\n",
      "Done with 5050000/8958024\n",
      "Done with 5075000/8958024\n",
      "Done with 5100000/8958024\n",
      "Done with 5125000/8958024\n",
      "Done with 5150000/8958024\n",
      "Done with 5175000/8958024\n",
      "Done with 5200000/8958024\n",
      "Done with 5225000/8958024\n",
      "Done with 5250000/8958024\n",
      "Done with 5275000/8958024\n",
      "Done with 5300000/8958024\n",
      "Done with 5325000/8958024\n",
      "Done with 5350000/8958024\n",
      "Done with 5375000/8958024\n",
      "Done with 5400000/8958024\n",
      "Done with 5425000/8958024\n",
      "Done with 5450000/8958024\n",
      "Done with 5475000/8958024\n",
      "Done with 5500000/8958024\n",
      "Done with 5525000/8958024\n",
      "Done with 5550000/8958024\n",
      "Done with 5575000/8958024\n",
      "Done with 5600000/8958024\n",
      "Done with 5625000/8958024\n",
      "Done with 5650000/8958024\n",
      "Done with 5675000/8958024\n",
      "Done with 5700000/8958024\n",
      "Done with 5725000/8958024\n",
      "Done with 5750000/8958024\n",
      "Done with 5775000/8958024\n",
      "Done with 5800000/8958024\n",
      "Done with 5825000/8958024\n",
      "Done with 5850000/8958024\n",
      "Done with 5875000/8958024\n",
      "Done with 5900000/8958024\n",
      "Done with 5925000/8958024\n",
      "Done with 5950000/8958024\n",
      "Done with 5975000/8958024\n",
      "Done with 6000000/8958024\n",
      "Done with 6025000/8958024\n",
      "Done with 6050000/8958024\n",
      "Done with 6075000/8958024\n",
      "Done with 6100000/8958024\n",
      "Done with 6125000/8958024\n",
      "Done with 6150000/8958024\n",
      "Done with 6175000/8958024\n",
      "Done with 6200000/8958024\n",
      "Done with 6225000/8958024\n",
      "Done with 6250000/8958024\n",
      "Done with 6275000/8958024\n",
      "Done with 6300000/8958024\n",
      "Done with 6325000/8958024\n",
      "Done with 6350000/8958024\n",
      "Done with 6375000/8958024\n",
      "Done with 6400000/8958024\n",
      "Done with 6425000/8958024\n",
      "Done with 6450000/8958024\n",
      "Done with 6475000/8958024\n",
      "Done with 6500000/8958024\n",
      "Done with 6525000/8958024\n",
      "Done with 6550000/8958024\n",
      "Done with 6575000/8958024\n",
      "Done with 6600000/8958024\n",
      "Done with 6625000/8958024\n",
      "Done with 6650000/8958024\n",
      "Done with 6675000/8958024\n",
      "Done with 6700000/8958024\n",
      "Done with 6725000/8958024\n",
      "Done with 6750000/8958024\n",
      "Done with 6775000/8958024\n",
      "Done with 6800000/8958024\n",
      "Done with 6825000/8958024\n",
      "Done with 6850000/8958024\n",
      "Done with 6875000/8958024\n",
      "Done with 6900000/8958024\n",
      "Done with 6925000/8958024\n",
      "Done with 6950000/8958024\n",
      "Done with 6975000/8958024\n",
      "Done with 7000000/8958024\n",
      "Done with 7025000/8958024\n",
      "Done with 7050000/8958024\n",
      "Done with 7075000/8958024\n",
      "Done with 7100000/8958024\n",
      "Done with 7125000/8958024\n",
      "Done with 7150000/8958024\n",
      "Done with 7175000/8958024\n",
      "Done with 7200000/8958024\n",
      "Done with 7225000/8958024\n",
      "Done with 7250000/8958024\n",
      "Done with 7275000/8958024\n",
      "Done with 7300000/8958024\n",
      "Done with 7325000/8958024\n",
      "Done with 7350000/8958024\n",
      "Done with 7375000/8958024\n",
      "Done with 7400000/8958024\n",
      "Done with 7425000/8958024\n",
      "Done with 7450000/8958024\n",
      "Done with 7475000/8958024\n",
      "Done with 7500000/8958024\n",
      "Done with 7525000/8958024\n",
      "Done with 7550000/8958024\n",
      "Done with 7575000/8958024\n",
      "Done with 7600000/8958024\n",
      "Done with 7625000/8958024\n",
      "Done with 7650000/8958024\n",
      "Done with 7675000/8958024\n",
      "Done with 7700000/8958024\n",
      "Done with 7725000/8958024\n",
      "Done with 7750000/8958024\n",
      "Done with 7775000/8958024\n",
      "Done with 7800000/8958024\n",
      "Done with 7825000/8958024\n",
      "Done with 7850000/8958024\n",
      "Done with 7875000/8958024\n",
      "Done with 7900000/8958024\n",
      "Done with 7925000/8958024\n",
      "Done with 7950000/8958024\n",
      "Done with 7975000/8958024\n",
      "Done with 8000000/8958024\n",
      "Done with 8025000/8958024\n",
      "Done with 8050000/8958024\n",
      "Done with 8075000/8958024\n",
      "Done with 8100000/8958024\n",
      "Done with 8125000/8958024\n",
      "Done with 8150000/8958024\n",
      "Done with 8175000/8958024\n",
      "Done with 8200000/8958024\n",
      "Done with 8225000/8958024\n",
      "Done with 8250000/8958024\n",
      "Done with 8275000/8958024\n",
      "Done with 8300000/8958024\n",
      "Done with 8325000/8958024\n",
      "Done with 8350000/8958024\n",
      "Done with 8375000/8958024\n",
      "Done with 8400000/8958024\n",
      "Done with 8425000/8958024\n",
      "Done with 8450000/8958024\n",
      "Done with 8475000/8958024\n",
      "Done with 8500000/8958024\n",
      "Done with 8525000/8958024\n",
      "Done with 8550000/8958024\n",
      "Done with 8575000/8958024\n",
      "Done with 8600000/8958024\n",
      "Done with 8625000/8958024\n",
      "Done with 8650000/8958024\n",
      "Done with 8675000/8958024\n",
      "Done with 8700000/8958024\n",
      "Done with 8725000/8958024\n",
      "Done with 8750000/8958024\n",
      "Done with 8775000/8958024\n",
      "Done with 8800000/8958024\n",
      "Done with 8825000/8958024\n",
      "Done with 8850000/8958024\n",
      "Done with 8875000/8958024\n",
      "Done with 8900000/8958024\n",
      "Done with 8925000/8958024\n",
      "Done with 8950000/8958024\n",
      "Done with 0/995336\n",
      "Done with 25000/995336\n",
      "Done with 50000/995336\n",
      "Done with 75000/995336\n",
      "Done with 100000/995336\n",
      "Done with 125000/995336\n",
      "Done with 150000/995336\n",
      "Done with 175000/995336\n",
      "Done with 200000/995336\n",
      "Done with 225000/995336\n",
      "Done with 250000/995336\n",
      "Done with 275000/995336\n",
      "Done with 300000/995336\n",
      "Done with 325000/995336\n",
      "Done with 350000/995336\n",
      "Done with 375000/995336\n",
      "Done with 400000/995336\n",
      "Done with 425000/995336\n",
      "Done with 450000/995336\n",
      "Done with 475000/995336\n",
      "Done with 500000/995336\n",
      "Done with 525000/995336\n",
      "Done with 550000/995336\n",
      "Done with 575000/995336\n",
      "Done with 600000/995336\n",
      "Done with 625000/995336\n",
      "Done with 650000/995336\n",
      "Done with 675000/995336\n",
      "Done with 700000/995336\n",
      "Done with 725000/995336\n",
      "Done with 750000/995336\n",
      "Done with 775000/995336\n",
      "Done with 800000/995336\n",
      "Done with 825000/995336\n",
      "Done with 850000/995336\n",
      "Done with 875000/995336\n",
      "Done with 900000/995336\n",
      "Done with 925000/995336\n",
      "Done with 950000/995336\n",
      "Done with 975000/995336\n"
     ]
    }
   ],
   "source": [
    "def transform_in_chunks(scaler, data, chunk_size):\n",
    "    count = 0\n",
    "    scaled_data = []\n",
    "    for chunk in chunkify(data, chunk_size):\n",
    "        print(f'Done with {count}/{len(data)}')\n",
    "        chunk_scaled = scaler.transform(chunk)\n",
    "        scaled_data.append(chunk_scaled)\n",
    "        count += chunk_size\n",
    "    return np.vstack(scaled_data)\n",
    "\n",
    "# Transform X_train and X_test in chunks\n",
    "X_train_scaled = transform_in_chunks(cpu_scaler, X_train, chunk_size)\n",
    "X_test_scaled = transform_in_chunks(cpu_scaler, X_test, chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train, X_test\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "cp.get_default_memory_pool().free_all_blocks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wont work, we dont have that much CUDA memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "#gpu_scaler = cumlStandardScaler()\n",
    "#X_train_scaled = gpu_scaler.fit_transform(X_train)\n",
    "#X_test_scaled = gpu_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 0/50\n",
      "Done with 1/50\n",
      "Done with 2/50\n",
      "Done with 3/50\n",
      "Done with 4/50\n",
      "Done with 5/50\n",
      "Done with 6/50\n",
      "Done with 7/50\n",
      "Done with 8/50\n",
      "Done with 9/50\n",
      "Done with 10/50\n",
      "Done with 11/50\n",
      "Done with 12/50\n",
      "Done with 13/50\n",
      "Done with 14/50\n",
      "Done with 15/50\n",
      "Done with 16/50\n",
      "Done with 17/50\n",
      "Done with 18/50\n",
      "Done with 19/50\n",
      "Done with 20/50\n",
      "Done with 21/50\n",
      "Done with 22/50\n",
      "Done with 23/50\n",
      "Done with 24/50\n",
      "Done with 25/50\n",
      "Done with 26/50\n",
      "Done with 27/50\n",
      "Done with 28/50\n",
      "Done with 29/50\n",
      "Done with 30/50\n",
      "Done with 31/50\n",
      "Done with 32/50\n",
      "Done with 33/50\n",
      "Done with 34/50\n",
      "Done with 35/50\n",
      "Done with 36/50\n",
      "Done with 37/50\n",
      "Done with 38/50\n",
      "Done with 39/50\n",
      "Done with 40/50\n",
      "Done with 41/50\n",
      "Done with 42/50\n",
      "Done with 43/50\n",
      "Done with 44/50\n",
      "Done with 45/50\n",
      "Done with 46/50\n",
      "Done with 47/50\n",
      "Done with 48/50\n",
      "Done with 49/50\n",
      "50\n",
      "286656\n"
     ]
    }
   ],
   "source": [
    "num_subsets = 50\n",
    "subsets = create_subsets(X_train_scaled, y_train, num_subsets, 1.6)\n",
    "\n",
    "print(len(subsets))\n",
    "print(len(subsets[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "cp.get_default_memory_pool().free_all_blocks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train SVR foreach subset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_models = []\n",
    "tqdm._instances.clear()\n",
    "numba.cuda.select_device(1)\n",
    "train = True\n",
    "if(os.path.exists('ensemble_svr_models.pkl')):\n",
    "    train = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVR model 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.9984\n",
      "  Precision: 1.0000\n",
      "  Recall: 0.8615\n",
      "  F1 Score: 0.9256\n",
      "Training SVR model 2/50\n",
      "  Accuracy: 0.9983\n",
      "  Precision: 0.9996\n",
      "  Recall: 0.8548\n",
      "  F1 Score: 0.9215\n",
      "Training SVR model 3/50\n"
     ]
    }
   ],
   "source": [
    "if(train):\n",
    "    for i, (X_subset, y_subset) in enumerate(subsets):\n",
    "        print(f'Training SVR model {i+1}/{num_subsets}')\n",
    "        svr = cumlSVR(kernel='rbf')\n",
    "        svr.fit(X_subset.astype(np.float32), y_subset.astype(np.float32))\n",
    "        log_progress(svr, X_subset, y_subset)\n",
    "        svr_models.append(svr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble of SVR models saved to ensemble_svr_models.pkl\n"
     ]
    }
   ],
   "source": [
    "# Save the ensemble\n",
    "if(train):\n",
    "    ensemble_filename = 'ensemble_svr_models.pkl'\n",
    "    with open(ensemble_filename, 'wb') as file:\n",
    "        pickle.dump(svr_models, file)\n",
    "\n",
    "    print(f'Ensemble of SVR models saved to {ensemble_filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble of SVR models loaded from file\n"
     ]
    }
   ],
   "source": [
    "# Load the ensemble of models from the file\n",
    "with open('ensemble_svr_models.pkl', 'rb') as file:\n",
    "    loaded_svr_models = pickle.load(file)\n",
    "\n",
    "print('Ensemble of SVR models loaded from file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction method for the ensemble:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predict(svr_models, X, threshold=0.5, proof=1):\n",
    "    predictions = np.zeros((len(svr_models), len(X)))\n",
    "    \n",
    "    for i, svr in enumerate(svr_models):\n",
    "        predictions[i] = svr.predict(X)\n",
    "    \n",
    "    # Count votes for each sample\n",
    "    votes = np.sum(predictions > threshold, axis=0)\n",
    "    \n",
    "    # If more than half the models predict 1, return 1; otherwise, return 0\n",
    "    final_predictions = (votes >= proof).astype(int)\n",
    "    return final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "std::bad_alloc: out_of_memory: CUDA error at: /home/staff_homes/kboenisc/miniconda3/envs/rapids-24.06/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Make predictions on the test set using the loaded ensemble of SVR models\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m y_pred_binary \u001b[38;5;241m=\u001b[39m \u001b[43mensemble_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloaded_svr_models\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_scaled\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[49], line 5\u001b[0m, in \u001b[0;36mensemble_predict\u001b[0;34m(svr_models, X, threshold, proof)\u001b[0m\n\u001b[1;32m      2\u001b[0m predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(svr_models), \u001b[38;5;28mlen\u001b[39m(X)))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, svr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(svr_models):\n\u001b[0;32m----> 5\u001b[0m     predictions[i] \u001b[38;5;241m=\u001b[39m \u001b[43msvr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Count votes for each sample\u001b[39;00m\n\u001b[1;32m      8\u001b[0m votes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(predictions \u001b[38;5;241m>\u001b[39m threshold, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids-24.06/lib/python3.10/site-packages/cuml/internals/api_decorators.py:188\u001b[0m, in \u001b[0;36m_make_decorator_function.<locals>.decorator_function.<locals>.decorator_closure.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    185\u001b[0m     set_api_output_dtype(output_dtype)\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m process_return:\n\u001b[0;32m--> 188\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32msvr.pyx:349\u001b[0m, in \u001b[0;36mcuml.svm.svr.SVR.predict\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids-24.06/lib/python3.10/site-packages/cuml/internals/api_decorators.py:188\u001b[0m, in \u001b[0;36m_make_decorator_function.<locals>.decorator_function.<locals>.decorator_closure.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    185\u001b[0m     set_api_output_dtype(output_dtype)\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m process_return:\n\u001b[0;32m--> 188\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32msvm_base.pyx:612\u001b[0m, in \u001b[0;36mcuml.svm.svm_base.SVMBase.predict\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids-24.06/lib/python3.10/site-packages/nvtx/nvtx.py:116\u001b[0m, in \u001b[0;36mannotate.__call__.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     libnvtx_push_range(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattributes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdomain\u001b[38;5;241m.\u001b[39mhandle)\n\u001b[0;32m--> 116\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m     libnvtx_pop_range(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdomain\u001b[38;5;241m.\u001b[39mhandle)\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids-24.06/lib/python3.10/site-packages/cuml/internals/input_utils.py:412\u001b[0m, in \u001b[0;36minput_to_cuml_array\u001b[0;34m(X, order, deepcopy, check_dtype, convert_to_dtype, check_mem_type, convert_to_mem_type, safe_dtype_conversion, check_cols, check_rows, fail_on_order, force_contiguous)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;129m@nvtx_annotate\u001b[39m(\n\u001b[1;32m    325\u001b[0m     message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcommon.input_utils.input_to_cuml_array\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    326\u001b[0m     category\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutils\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    341\u001b[0m     force_contiguous\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    342\u001b[0m ):\n\u001b[1;32m    343\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;124;03m    Convert input X to CumlArray.\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    410\u001b[0m \n\u001b[1;32m    411\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 412\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[43mCumlArray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_input\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeepcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_to_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_to_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_mem_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_mem_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_to_mem_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_to_mem_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43msafe_dtype_conversion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_dtype_conversion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfail_on_order\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfail_on_order\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_contiguous\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_contiguous\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    426\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    427\u001b[0m         shape \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39m__cuda_array_interface__[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids-24.06/lib/python3.10/site-packages/cuml/internals/memory_utils.py:87\u001b[0m, in \u001b[0;36mwith_cupy_rmm.<locals>.cupy_rmm_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m GPU_ENABLED:\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m cupy_using_allocator(rmm_cupy_allocator):\n\u001b[0;32m---> 87\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids-24.06/lib/python3.10/site-packages/nvtx/nvtx.py:116\u001b[0m, in \u001b[0;36mannotate.__call__.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     libnvtx_push_range(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattributes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdomain\u001b[38;5;241m.\u001b[39mhandle)\n\u001b[0;32m--> 116\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m     libnvtx_pop_range(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdomain\u001b[38;5;241m.\u001b[39mhandle)\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids-24.06/lib/python3.10/site-packages/cuml/internals/array.py:1171\u001b[0m, in \u001b[0;36mCumlArray.from_input\u001b[0;34m(cls, X, order, deepcopy, check_dtype, convert_to_dtype, check_mem_type, convert_to_mem_type, safe_dtype_conversion, check_cols, check_rows, fail_on_order, force_contiguous)\u001b[0m\n\u001b[1;32m   1165\u001b[0m make_copy \u001b[38;5;241m=\u001b[39m force_contiguous \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mis_contiguous\n\u001b[1;32m   1167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1168\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m fail_on_order \u001b[38;5;129;01mand\u001b[39;00m order \u001b[38;5;241m!=\u001b[39m arr\u001b[38;5;241m.\u001b[39morder \u001b[38;5;129;01mand\u001b[39;00m order \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mK\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1169\u001b[0m ) \u001b[38;5;129;01mor\u001b[39;00m make_copy:\n\u001b[1;32m   1170\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\n\u001b[0;32m-> 1171\u001b[0m         \u001b[43marr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmem_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1172\u001b[0m \u001b[43m            \u001b[49m\u001b[43marr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_output\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marray\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_copy\u001b[49m\n\u001b[1;32m   1173\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1174\u001b[0m         index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[1;32m   1175\u001b[0m     )\n\u001b[1;32m   1177\u001b[0m n_rows \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(arr\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids-24.06/lib/python3.10/site-packages/cupy/_creation/from_data.py:53\u001b[0m, in \u001b[0;36marray\u001b[0;34m(obj, dtype, copy, order, subok, ndmin, blocking)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21marray\u001b[39m(obj, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mK\u001b[39m\u001b[38;5;124m'\u001b[39m, subok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, ndmin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m      8\u001b[0m           blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m      9\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Creates an array on the current device.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m    This function currently does not support the ``subok`` option.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m \n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_core\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubok\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndmin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32mcupy/_core/core.pyx:2383\u001b[0m, in \u001b[0;36mcupy._core.core.array\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mcupy/_core/core.pyx:2393\u001b[0m, in \u001b[0;36mcupy._core.core.array\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mcupy/_core/core.pyx:2424\u001b[0m, in \u001b[0;36mcupy._core.core._array_from_cupy_ndarray\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mcupy/_core/core.pyx:516\u001b[0m, in \u001b[0;36mcupy._core.core._ndarray_base.astype\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mcupy/_core/core.pyx:574\u001b[0m, in \u001b[0;36mcupy._core.core._ndarray_base.astype\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mcupy/_core/core.pyx:135\u001b[0m, in \u001b[0;36mcupy._core.core.ndarray.__new__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mcupy/_core/core.pyx:223\u001b[0m, in \u001b[0;36mcupy._core.core._ndarray_base._init\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mcupy/cuda/memory.pyx:738\u001b[0m, in \u001b[0;36mcupy.cuda.memory.alloc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids-24.06/lib/python3.10/site-packages/rmm/allocators/cupy.py:37\u001b[0m, in \u001b[0;36mrmm_cupy_allocator\u001b[0;34m(nbytes)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo module named \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcupy\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m stream \u001b[38;5;241m=\u001b[39m Stream(obj\u001b[38;5;241m=\u001b[39mcupy\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mget_current_stream())\n\u001b[0;32m---> 37\u001b[0m buf \u001b[38;5;241m=\u001b[39m \u001b[43mlibrmm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDeviceBuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m dev_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m buf\u001b[38;5;241m.\u001b[39mptr \u001b[38;5;28;01melse\u001b[39;00m cupy\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mget_device_id()\n\u001b[1;32m     39\u001b[0m mem \u001b[38;5;241m=\u001b[39m cupy\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mUnownedMemory(\n\u001b[1;32m     40\u001b[0m     ptr\u001b[38;5;241m=\u001b[39mbuf\u001b[38;5;241m.\u001b[39mptr, size\u001b[38;5;241m=\u001b[39mbuf\u001b[38;5;241m.\u001b[39msize, owner\u001b[38;5;241m=\u001b[39mbuf, device_id\u001b[38;5;241m=\u001b[39mdev_id\n\u001b[1;32m     41\u001b[0m )\n",
      "File \u001b[0;32mdevice_buffer.pyx:96\u001b[0m, in \u001b[0;36mrmm._lib.device_buffer.DeviceBuffer.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: std::bad_alloc: out_of_memory: CUDA error at: /home/staff_homes/kboenisc/miniconda3/envs/rapids-24.06/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set using the loaded ensemble of SVR models\n",
    "y_pred_binary = ensemble_predict(loaded_svr_models, X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.9966\n",
      "Precision: 0.9987\n",
      "Recall: 0.8527\n",
      "F1-score: 0.9199\n",
      "Confusion Matrix:\n",
      "[[491902     13]\n",
      " [  1697   9824]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    491915\n",
      "           1       1.00      0.85      0.92     11521\n",
      "\n",
      "    accuracy                           1.00    503436\n",
      "   macro avg       1.00      0.93      0.96    503436\n",
      "weighted avg       1.00      1.00      1.00    503436\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "precision = precision_score(y_test, y_pred_binary)\n",
    "recall = recall_score(y_test, y_pred_binary)\n",
    "f1 = f1_score(y_test, y_pred_binary)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_binary)\n",
    "class_report = classification_report(y_test, y_pred_binary)\n",
    "\n",
    "print(f'Test set accuracy: {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1-score: {f1:.4f}')\n",
    "print(f'Confusion Matrix:\\n{conf_matrix}')\n",
    "print(f'Classification Report:\\n{class_report}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Relevant', 'Relevant'], yticklabels=['Not Relevant', 'Relevant'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Plotting SVR results (for regression)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m----> 3\u001b[0m plt\u001b[38;5;241m.\u001b[39mscatter(y_test, \u001b[43my_pred\u001b[49m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m'\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSVR Predictions\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot([y_test\u001b[38;5;241m.\u001b[39mmin(), y_test\u001b[38;5;241m.\u001b[39mmax()], [y_test\u001b[38;5;241m.\u001b[39mmin(), y_test\u001b[38;5;241m.\u001b[39mmax()], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mk--\u001b[39m\u001b[38;5;124m'\u001b[39m, lw\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPerfect Prediction\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSVR Predictions vs Actual\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_pred' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting SVR results (for regression)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, color='blue', alpha=0.3, label='SVR Predictions')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2, label='Perfect Prediction')\n",
    "plt.title('SVR Predictions vs Actual')\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
